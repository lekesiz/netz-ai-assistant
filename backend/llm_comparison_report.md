# ğŸ¤– NETZ AI - LLM Model KarÅŸÄ±laÅŸtÄ±rma Raporu

## ğŸ’» Sistem Ã–zellikleri
- **RAM**: 128GB
- **Ä°ÅŸlemci**: M4-Max (CPU+GPU)
- **Disk**: 2TB
- **Platform**: macOS (Apple Silicon)

## ğŸ“Š DetaylÄ± Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

### 1. **Mistral 7B** (Mevcut)
```
Boyut: 4.4 GB | HÄ±z: âš¡âš¡âš¡âš¡âš¡ | DoÄŸruluk: â­â­â­â­
```
**Avantajlar:**
- âœ… MÃ¼kemmel hÄ±z/kalite dengesi
- âœ… Ã‡ok dilli destek (TR, FR, EN)
- âœ… DÃ¼ÅŸÃ¼k RAM kullanÄ±mÄ± (~6GB)
- âœ… M4-Max ile tam uyumlu
- âœ… Ticari kullanÄ±ma aÃ§Ä±k lisans

**Dezavantajlar:**
- âŒ Kodlama iÃ§in orta seviye
- âŒ Matematik/mantÄ±k sÄ±nÄ±rlÄ±
- âŒ Context window: 32K token

**KullanÄ±m Senaryosu:** Genel sohbet, mÃ¼ÅŸteri desteÄŸi, basit teknik sorular

---

### 2. **Qwen 2.5 Coder 32B** 
```
Boyut: 19 GB | HÄ±z: âš¡âš¡âš¡ | DoÄŸruluk: â­â­â­â­â­
```
**Avantajlar:**
- âœ… Kodlama iÃ§in optimize
- âœ… 100+ programlama dili
- âœ… Debugging yetenekleri
- âœ… Context window: 128K token
- âœ… TÃ¼rkÃ§e desteÄŸi iyi

**Dezavantajlar:**
- âŒ YÃ¼ksek RAM kullanÄ±mÄ± (~24GB)
- âŒ Daha yavaÅŸ yanÄ±t
- âŒ Genel konularda zayÄ±f

**KullanÄ±m Senaryosu:** Kod yazma, debug, teknik dokÃ¼mantasyon

---

### 3. **DeepSeek Coder v2**
```
Boyut: 8.9 GB | HÄ±z: âš¡âš¡âš¡âš¡ | DoÄŸruluk: â­â­â­â­â­
```
**Avantajlar:**
- âœ… Kod tamamlama Ã§ok gÃ¼Ã§lÃ¼
- âœ… Fill-in-the-middle desteÄŸi
- âœ… Repository-level kod anlama
- âœ… HÄ±zlÄ± inference

**Dezavantajlar:**
- âŒ TÃ¼rkÃ§e desteÄŸi zayÄ±f
- âŒ Genel konularda yetersiz
- âŒ Ã‡in menÅŸeli (gÃ¼venlik?)

**KullanÄ±m Senaryosu:** IDE entegrasyonu, kod tamamlama

---

### 4. **Llama 3.2** 
```
Boyut: 2.0 GB | HÄ±z: âš¡âš¡âš¡âš¡âš¡ | DoÄŸruluk: â­â­â­
```
**Avantajlar:**
- âœ… Ã‡ok hÄ±zlÄ±
- âœ… DÃ¼ÅŸÃ¼k kaynak kullanÄ±mÄ±
- âœ… Meta desteÄŸi
- âœ… Vision yetenekleri

**Dezavantajlar:**
- âŒ TÃ¼rkÃ§e desteÄŸi zayÄ±f
- âŒ KÃ¼Ã§Ã¼k model, sÄ±nÄ±rlÄ± yetenekler
- âŒ HallÃ¼sinasyon oranÄ± yÃ¼ksek

**KullanÄ±m Senaryosu:** HÄ±zlÄ± prototipleme, basit gÃ¶revler

---

### 5. **Qwen 2.5 72B**
```
Boyut: 47 GB | HÄ±z: âš¡âš¡ | DoÄŸruluk: â­â­â­â­â­
```
**Avantajlar:**
- âœ… En yÃ¼ksek doÄŸruluk
- âœ… MÃ¼kemmel TÃ¼rkÃ§e
- âœ… KarmaÅŸÄ±k reasoning
- âœ… Uzun context (128K)

**Dezavantajlar:**
- âŒ Ã‡ok yavaÅŸ (~5-10 sn/yanÄ±t)
- âŒ YÃ¼ksek RAM (~60GB)
- âŒ GPU yoÄŸun kullanÄ±m

**KullanÄ±m Senaryosu:** KarmaÅŸÄ±k analizler, araÅŸtÄ±rma

---

### 6. **CodeLlama 34B**
```
Boyut: 19 GB | HÄ±z: âš¡âš¡âš¡ | DoÄŸruluk: â­â­â­â­
```
**Avantajlar:**
- âœ… Meta'nÄ±n kod modeli
- âœ… Ä°yi dokÃ¼mantasyon
- âœ… GÃ¼venilir Ã§Ä±ktÄ±lar
- âœ… Python'da Ã§ok gÃ¼Ã§lÃ¼

**Dezavantajlar:**
- âŒ TÃ¼rkÃ§e yok
- âŒ Sadece kod odaklÄ±
- âŒ Eski model (Llama2 bazlÄ±)

**KullanÄ±m Senaryosu:** Kod review, refactoring

---

## ğŸ¯ M4-Max iÃ§in Ã–neriler

### En Ä°yi Performans: **Mistral 7B + Qwen 2.5 Coder 32B Kombinasyonu**
```yaml
Genel Sorular: Mistral 7B
Kod SorularÄ±: Qwen 2.5 Coder 32B
Ortalama YanÄ±t: <1 saniye
RAM KullanÄ±mÄ±: ~30GB toplam
```

### Alternatif Setup: **DeepSeek Coder v2 + Gemma 3 27B**
```yaml
Teknik: DeepSeek Coder v2
Genel: Gemma 3 27B
Ortalama YanÄ±t: 1-2 saniye
RAM KullanÄ±mÄ±: ~35GB toplam
```

---

## ğŸ”„ Dinamik Model DeÄŸiÅŸtirme Sistemi

### Implementasyon Ã–nerisi:

```python
class ModelSelector:
    def __init__(self):
        self.models = {
            "general": "mistral:latest",
            "coding": "qwen2.5-coder:32b",
            "fast": "llama3.2:latest",
            "accurate": "qwen2.5:72b"
        }
    
    def select_model(self, query_type, user_preference):
        # Otomatik model seÃ§imi
        if "code" in query_type:
            return self.models["coding"]
        elif user_preference == "fast":
            return self.models["fast"]
        elif user_preference == "accurate":
            return self.models["accurate"]
        else:
            return self.models["general"]
```

### KullanÄ±cÄ± ArayÃ¼zÃ¼:
```
ğŸ¤– Model SeÃ§imi:
[ ] HÄ±zlÄ± YanÄ±t (Llama 3.2)
[x] Dengeli (Mistral 7B)
[ ] Kod UzmanÄ± (Qwen Coder)
[ ] Maksimum DoÄŸruluk (Qwen 72B)
[ ] Otomatik SeÃ§
```

---

## ğŸ’¾ HafÄ±za ve Model Ä°liÅŸkisi

### Her Model iÃ§in HafÄ±za Gereksinimleri:
| Model | RAM (Idle) | RAM (Active) | VRAM | Context Window |
|-------|------------|--------------|------|----------------|
| Mistral 7B | 6GB | 8GB | 4GB | 32K tokens |
| Qwen 2.5 Coder 32B | 20GB | 24GB | 12GB | 128K tokens |
| DeepSeek v2 | 10GB | 12GB | 6GB | 64K tokens |
| Llama 3.2 | 3GB | 4GB | 2GB | 16K tokens |
| Qwen 72B | 50GB | 60GB | 20GB | 128K tokens |

### Context Window ve Ã–ÄŸrenme:
- **KÃ¼Ã§Ã¼k context** (16K): KÄ±sa sohbetler, hÄ±zlÄ± unutma
- **Orta context** (32K): Normal kullanÄ±m, iyi denge
- **BÃ¼yÃ¼k context** (128K): Uzun dÃ¶kÃ¼manlar, detaylÄ± hafÄ±za

---

## ğŸš€ Performans Optimizasyonu

### M4-Max iÃ§in Ã–zel Ayarlar:
```bash
# GPU hÄ±zlandÄ±rma
export OLLAMA_NUM_GPU=1
export OLLAMA_GPU_LAYERS=40

# CPU optimizasyonu
export OLLAMA_NUM_THREAD=20
export OLLAMA_COMPUTE_UNIT=GPU

# Bellek yÃ¶netimi
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_KEEP_ALIVE=5m
```

### Model Preloading:
```python
# SÄ±k kullanÄ±lan modelleri bellekte tut
preload_models = ["mistral:latest", "qwen2.5-coder:latest"]
for model in preload_models:
    ollama.pull(model)
    ollama.generate(model, "test", keep_alive=300)
```

---

## ğŸ“ˆ Benchmark SonuÃ§larÄ± (M4-Max)

| Model | Ä°lk YanÄ±t | Token/sn | DoÄŸruluk | TÃ¼rkÃ§e |
|-------|-----------|----------|----------|---------|
| Mistral 7B | 0.3s | 120 t/s | 85% | 90% |
| Qwen Coder 32B | 1.2s | 40 t/s | 95% | 85% |
| DeepSeek v2 | 0.5s | 80 t/s | 92% | 60% |
| Llama 3.2 | 0.2s | 150 t/s | 75% | 50% |
| Qwen 72B | 3.5s | 20 t/s | 98% | 95% |

---

## ğŸ’¡ SonuÃ§ ve Ã–neriler

### NETZ iÃ§in Optimal Setup:
1. **Ana Model**: Mistral 7B (genel kullanÄ±m)
2. **Kod AsistanÄ±**: Qwen 2.5 Coder 32B
3. **HÄ±zlÄ± Mod**: Llama 3.2 (demo/test)
4. **AraÅŸtÄ±rma Modu**: Qwen 72B (haftalÄ±k raporlar)

### KullanÄ±cÄ± Deneyimi:
- Default: Mistral 7B
- "Kod yaz" algÄ±landÄ±ÄŸÄ±nda: Otomatik Qwen Coder
- KullanÄ±cÄ± "hÄ±zlÄ± yanÄ±t" seÃ§erse: Llama 3.2
- Admin "detaylÄ± analiz" isterse: Qwen 72B

### Sistem Etkisi:
- **RAM KullanÄ±mÄ±**: Max 60GB (tÃ¼m modeller yÃ¼klÃ¼)
- **YanÄ±t SÃ¼resi**: 0.2s - 3.5s arasÄ±
- **GPU KullanÄ±mÄ±**: %20-80 arasÄ±
- **Disk**: ~150GB (tÃ¼m modeller)

Bu setup ile M4-Max'in gÃ¼cÃ¼nÃ¼ maksimum kullanarak, kullanÄ±cÄ± ihtiyacÄ±na gÃ¶re dinamik model seÃ§imi yapabilirsiniz.